{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "for model selection and results analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from scripts.utils import initialize_bucket, plot_importance\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import KFold, cross_val_score,cross_validate\n",
    "from sklearn.metrics import r2_score, mean_squared_error, make_scorer, explained_variance_score,mean_absolute_percentage_error\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import shap\n",
    "\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 400)\n",
    "\n",
    "credentials_path = '../datascience-capstone-project-05b1642f45c3.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client, bucket = initialize_bucket(credentials_path,'storage-barsianize')\n",
    "\n",
    "path = \"gs://storage-barsianize/05_datasets/df_base_dataset.parquet\"\n",
    "df =  pd.read_parquet(path)\n",
    "\n",
    "path = \"gs://storage-barsianize/05_datasets/df_to_pred.parquet\"\n",
    "df_to_pred =  pd.read_parquet(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path, model_name):\n",
    "    \n",
    "    with open(path + model_name, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "\n",
    "    loaded_model = None\n",
    "    for step_name, step_model in model.named_steps.items():\n",
    "        if isinstance(step_model, LGBMRegressor):\n",
    "            loaded_model = step_model\n",
    "            break\n",
    "        elif isinstance(step_model, XGBRegressor):\n",
    "            loaded_model = step_model\n",
    "            break\n",
    "        elif step_name == 'preprocessing':\n",
    "            transformer = step_model\n",
    "\n",
    "    return loaded_model, transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(path, model_name, results_file, X_to_pred):\n",
    "    # model_name = '2023-08-04_LGBMRegressor_Normalizer()_feat_selection.pkl'\n",
    "    with open(path + model_name, 'rb') as file:\n",
    "        pipeline = pickle.load(file)\n",
    "\n",
    "    with open(path + results_file, 'rb') as json_file:\n",
    "        results = json.load(json_file)\n",
    "\n",
    "    model, transformer = load_model(path, model_name)\n",
    "\n",
    "    features = results['features']\n",
    "\n",
    "    return pipeline.predict(X_to_pred[features]), pipeline, model, transformer, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importances(model, model_class, features):\n",
    "    if 'LGB' in model_class:\n",
    "        feature_importances = model.booster_.feature_importance(importance_type='gain')\n",
    "    elif 'XGB' in model_class:\n",
    "        feature_importances = model.get_booster().get_score(importance_type='gain').values()\n",
    "        \n",
    "    # Create a dictionary to associate feature names with their importance scores\n",
    "    feature_importance_dict = dict(zip(features, feature_importances))\n",
    "    df_feature_importance = pd.DataFrame(feature_importance_dict, \n",
    "                                      index=['Total gain']).T.sort_values(by='Total gain', ascending=False)\n",
    "    return df_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_shap_values(model, transformer, X_train, X_test, path, filename, plot=False):\n",
    "    # If the LightGBM model is found, convert it to a LightGBM Booster object and prepare the explainer\n",
    "    if model is not None:\n",
    "        # Assuming you have your test data in 'X_test' (replace 'X_test' with your actual test data)\n",
    "        explainer = shap.Explainer(model, transformer.transform(X_train))\n",
    "\n",
    "        # Calculate SHAP values for the test data\n",
    "        shap_values = explainer(transformer.transform(X_test), check_additivity=False)\n",
    "\n",
    "        # Save SHAP values to a file using pickle\n",
    "        with open(path + filename, 'wb') as file:\n",
    "            pickle.dump(shap_values, file)\n",
    "\n",
    "        # plot shap summary if desired\n",
    "        if plot:\n",
    "            shap.summary_plot(shap_values, X_test)\n",
    "        \n",
    "        # Return SHAP values\n",
    "        return shap_values\n",
    "    \n",
    "    else:\n",
    "        print(\"LightGBM model not found in the pipeline.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dy_metrics(data, sort_col, filter_col,value_filter):\n",
    "\n",
    "    data = data.sort_values(by=sort_col, ascending=False)\n",
    "    \n",
    "    d = {}\n",
    "    d['top5'] = {\n",
    "                        'dy_mean'     : data['dy_label'].iloc[:5].mean(),\n",
    "                        'pl_mean_last'     : data['pl_mean_last'].iloc[:5].mean(),\n",
    "                        'cotacao_mean_last': data['cotacao_mean_last'].iloc[:5].mean()\n",
    "                    }\n",
    "\n",
    "    d['top10'] = {\n",
    "                        'dy_mean'     : data['dy_label'].iloc[:10].mean(),\n",
    "                        'pl_mean_last'     : data['pl_mean_last'].iloc[:10].mean(),\n",
    "                        'cotacao_mean_last': data['cotacao_mean_last'].iloc[:10].mean()\n",
    "                    }\n",
    "\n",
    "    d['top20'] = {\n",
    "                        'dy_mean'     : data['dy_label'].iloc[:20].mean(),\n",
    "                        'pl_mean_last'     : data['pl_mean_last'].iloc[:20].mean(),\n",
    "                        'cotacao_mean_last': data['cotacao_mean_last'].iloc[:20].mean()\n",
    "                    }\n",
    "\n",
    "    d[f'top5_{filter_col}'] = {\n",
    "                        'dy_mean'     : data[data[filter_col]==value_filter]['dy_label'].iloc[:5].mean(),\n",
    "                        'pl_mean_last'     : data[data[filter_col]==value_filter]['pl_mean_last'].iloc[:5].mean(),\n",
    "                        'cotacao_mean_last': data[data[filter_col]==value_filter]['cotacao_mean_last'].iloc[:5].mean()\n",
    "                    }\n",
    "\n",
    "    d[f'top10_{filter_col}'] = {\n",
    "                        'dy_mean'     : data[data[filter_col]==value_filter]['dy_label'].iloc[:10].mean(),\n",
    "                        'pl_mean_last'     : data[data[filter_col]==value_filter]['pl_mean_last'].iloc[:10].mean(),\n",
    "                        'cotacao_mean_last': data[data[filter_col]==value_filter]['cotacao_mean_last'].iloc[:10].mean()\n",
    "                    }\n",
    "\n",
    "    d[f'top20_{filter_col}'] = {\n",
    "                        'dy_mean'     : data[data[filter_col]==value_filter]['dy_label'].iloc[:20].mean(),\n",
    "                        'pl_mean_last'     : data[data[filter_col]==value_filter]['pl_mean_last'].iloc[:20].mean(),\n",
    "                        'cotacao_mean_last': data[data[filter_col]==value_filter]['cotacao_mean_last'].iloc[:20].mean()\n",
    "                    }\n",
    "\n",
    "    return pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(data, y, title, palette):\n",
    "    plt.figure(figsize=(20,20), dpi=400)\n",
    "    ax = sns.barplot(x=data.sort_values(y).index,\n",
    "                    y=y, palette=palette, data=data.sort_values(y))\n",
    "    # ax = sns.barplot(x=df_grupos_de_despesas.index[:-1],\n",
    "    #                  y='Valor', palette='viridis', data=df_grupos_de_despesas[:-1])\n",
    "    plt.xlabel('Model Output',fontsize=24)\n",
    "    plt.ylabel('DY [-]', fontsize=20)\n",
    "    plt.title(title, fontsize=24)\n",
    "    plt.xticks(rotation=90, fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "\n",
    "    for p in ax.patches:\n",
    "        ax.annotate('{:,.3f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.005), fontsize=20,rotation=0)\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def create_syled_sheet(data, filter_word_in, sort_col, cmap):\n",
    "    return data.sort_values(sort_col, ascending=False)[[col for col in data.columns if filter_word_in in col]]\\\n",
    "        .style.background_gradient(cmap=cmap,axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse the prediction outcomes for each model\n",
    "### Best models (Transformer + Algorithm + Feature Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating the train and target features\n",
    "X_to_pred = df_to_pred.drop(['year_month','Papel','Empresa','dy_label'], axis=1)\n",
    "y_to_pred = df_to_pred['dy_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/03_models/out/'\n",
    "\n",
    "# '2023-08-07_LGBMRegressor_Normalizer()_feat_selection.pkl'\n",
    "\n",
    "model_name = '2023-08-07_LGBMRegressor_Normalizer()_feat_selection.pkl'\n",
    "results_file = '2023-08-07_LGBMRegressor_Normalizer()_results_feat_selection.json'\n",
    "file_name_SHAP = '2023-08-07_LGBMRegressor_Normalizer()_results_feat_selection_SHAP.pkl'\n",
    "\n",
    "df_to_pred['dy_pred_N_FS'], N_FS_0807_pipeline, N_FS_0807_model, N_FS_0807_transformer, N_FS_0807_features = make_predictions(path, model_name, results_file, X_to_pred)\n",
    "\n",
    "X = df[N_FS_0807_features]\n",
    "y = df['dy_label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=32)\n",
    "\n",
    "N_FS_0807_shap = calculate_shap_values(N_FS_0807_model, N_FS_0807_transformer, X_train, X_test, path, file_name_SHAP, plot=True)\n",
    "\n",
    "\n",
    "\n",
    "# '2023-08-07_LGBMRegressor_StandardScaler()_feat_selection.pkl' \n",
    "\n",
    "model_name = '2023-08-07_LGBMRegressor_StandardScaler()_feat_selection.pkl'\n",
    "results_file = '2023-08-07_LGBMRegressor_StandardScaler()_results_feat_selection.json'\n",
    "file_name_SHAP = '2023-08-07_LGBMRegressor_StandardScaler()_results_feat_selection_SHAP.pkl'\n",
    "\n",
    "df_to_pred['dy_pred_S_FS'], S_FS_0807_pipeline, S_FS_0807_model, S_FS_0807_transformer, S_FS_0807_features = make_predictions(path, model_name, results_file, X_to_pred)\n",
    "\n",
    "X = df[S_FS_0807_features]\n",
    "y = df['dy_label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=32)\n",
    "\n",
    "S_FS_0807_shap = calculate_shap_values(S_FS_0807_model, N_FS_0807_transformer, X_train, X_test, path, file_name_SHAP, plot=True)\n",
    "\n",
    "\n",
    "\n",
    "path = '../data/03_models/out/'\n",
    "\n",
    "# '2023-08-05_LGBMRegressor_Normalizer()_feat_selection_lgbm.pkl'\n",
    "\n",
    "model_name = '2023-08-07_LGBMRegressor_Normalizer()_feat_selection_lgbm.pkl'\n",
    "results_file = '2023-08-07_LGBMRegressor_Normalizer()_results_feat_selection_lgbm.json'\n",
    "file_name_SHAP = '2023-08-07_LGBMRegressor_Normalizer()_results_feat_selection_lgbm_SHAP.pkl'\n",
    "\n",
    "df_to_pred['dy_pred_N_FS_LGBM'], N_FS_LGBM_0807_pipeline, N_FS_LGBM_0807_model, N_FS_LGBM_0807_transformer, N_FS_LGBM_0807_features = make_predictions(path, model_name, results_file, X_to_pred)\n",
    "\n",
    "X = df[N_FS_LGBM_0807_features]\n",
    "y = df['dy_label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=32)\n",
    "\n",
    "N_FS_LGBM_0807_shap = calculate_shap_values(N_FS_LGBM_0807_model, N_FS_LGBM_0807_transformer, X_train, X_test, path, file_name_SHAP, plot=True)\n",
    "\n",
    "\n",
    "\n",
    "# '2023-08-07_LGBMRegressor_StandardScaler()_feat_selection_lgbm.pkl' \n",
    "\n",
    "model_name = '2023-08-07_LGBMRegressor_StandardScaler()_feat_selection_lgbm.pkl'\n",
    "results_file = '2023-08-07_LGBMRegressor_StandardScaler()_results_feat_selection_lgbm.json'\n",
    "file_name_SHAP = '2023-08-07_LGBMRegressor_StandardScaler()_results_feat_selection_lgbm_SHAP.pkl'\n",
    "\n",
    "df_to_pred['dy_pred_S_FS_LGBM'], S_FS_LGBM_0807_pipeline, S_FS_LGBM_0807_model, S_FS_LGBM_0807_transformer, S_FS_LGBM_0807_features = make_predictions(path, model_name, results_file, X_to_pred)\n",
    "\n",
    "X = df[S_FS_LGBM_0807_features]\n",
    "y = df['dy_label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=32)\n",
    "\n",
    "S_FS_LGBM_0807_shap = calculate_shap_values(S_FS_LGBM_0807_model, S_FS_LGBM_0807_transformer, X_train, X_test, path, file_name_SHAP, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/03_models/out/'\n",
    "\n",
    "# '2023-08-08_LGBMRegressor_Normalizer()_feat_selection.pkl'\n",
    "\n",
    "model_name = '2023-08-08_LGBMRegressor_Normalizer()_feat_selection.pkl'\n",
    "results_file = '2023-08-08_LGBMRegressor_Normalizer()_results_feat_selection.json'\n",
    "file_name_SHAP = '2023-08-08_LGBMRegressor_Normalizer()_results_feat_selection_SHAP.pkl'\n",
    "\n",
    "df_to_pred['dy_pred_N_FS_0808'], N_FS_0808_pipeline, N_FS_0808_model, N_FS_0808_transformer, N_FS_0808_features = make_predictions(path, model_name, results_file, X_to_pred)\n",
    "\n",
    "X = df[N_FS_0808_features]\n",
    "y = df['dy_label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=32)\n",
    "\n",
    "N_FS_0808_shap = calculate_shap_values(N_FS_0808_model, N_FS_0808_transformer, X_train, X_test, path, file_name_SHAP, plot=True)\n",
    "\n",
    "\n",
    "\n",
    "# '2023-08-08_LGBMRegressor_StandardScaler()_feat_selection.pkl' \n",
    "\n",
    "model_name = '2023-08-08_LGBMRegressor_StandardScaler()_feat_selection.pkl'\n",
    "results_file = '2023-08-08_LGBMRegressor_StandardScaler()_results_feat_selection.json'\n",
    "file_name_SHAP = '2023-08-08_LGBMRegressor_StandardScaler()_results_feat_selection_SHAP.pkl'\n",
    "\n",
    "df_to_pred['dy_pred_S_FS_0808'], S_FS_0808_pipeline, S_FS_0808_model, S_FS_0808_transformer, S_FS_0808_features = make_predictions(path, model_name, results_file, X_to_pred)\n",
    "\n",
    "X = df[S_FS_0808_features]\n",
    "y = df['dy_label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=32)\n",
    "\n",
    "S_FS_0808_shap = calculate_shap_values(S_FS_0808_model, N_FS_0808_transformer, X_train, X_test, path, file_name_SHAP, plot=True)\n",
    "\n",
    "\n",
    "\n",
    "path = '../data/03_models/out/'\n",
    "\n",
    "# '2023-08-05_LGBMRegressor_Normalizer()_feat_selection_lgbm.pkl'\n",
    "\n",
    "model_name = '2023-08-08_LGBMRegressor_Normalizer()_feat_selection_lgbm.pkl'\n",
    "results_file = '2023-08-08_LGBMRegressor_Normalizer()_results_feat_selection_lgbm.json'\n",
    "file_name_SHAP = '2023-08-08_LGBMRegressor_Normalizer()_results_feat_selection_lgbm_SHAP.pkl'\n",
    "\n",
    "df_to_pred['dy_pred_N_FS_LGBM_0808'], N_FS_LGBM_0808_pipeline, N_FS_LGBM_0808_model, N_FS_LGBM_0808_transformer, N_FS_LGBM_0808_features = make_predictions(path, model_name, results_file, X_to_pred)\n",
    "\n",
    "X = df[N_FS_LGBM_0808_features]\n",
    "y = df['dy_label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=32)\n",
    "\n",
    "N_FS_LGBM_0808_shap = calculate_shap_values(N_FS_LGBM_0808_model, N_FS_LGBM_0808_transformer, X_train, X_test, path, file_name_SHAP, plot=True)\n",
    "\n",
    "\n",
    "\n",
    "# '2023-08-08_LGBMRegressor_StandardScaler()_feat_selection_lgbm.pkl' \n",
    "\n",
    "model_name = '2023-08-08_LGBMRegressor_StandardScaler()_feat_selection_lgbm.pkl'\n",
    "results_file = '2023-08-08_LGBMRegressor_StandardScaler()_results_feat_selection_lgbm.json'\n",
    "file_name_SHAP = '2023-08-08_LGBMRegressor_StandardScaler()_results_feat_selection_lgbm_SHAP.pkl'\n",
    "\n",
    "df_to_pred['dy_pred_S_FS_LGBM_0808'], S_FS_LGBM_0808_pipeline, S_FS_LGBM_0808_model, S_FS_LGBM_0808_transformer, S_FS_LGBM_0808_features = make_predictions(path, model_name, results_file, X_to_pred)\n",
    "\n",
    "X = df[S_FS_LGBM_0808_features]\n",
    "y = df['dy_label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=32)\n",
    "\n",
    "S_FS_LGBM_0808_shap = calculate_shap_values(S_FS_LGBM_0808_model, S_FS_LGBM_0808_transformer, X_train, X_test, path, file_name_SHAP, plot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend 20 best Stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_pred['Papel_root'] = df_to_pred['Papel'].str[:4]\n",
    "\n",
    "df_recommend = df_to_pred[['Empresa','Papel','Papel_root','besst_1','besst_2','dy_label','pl_mean_last','cotacao_mean_last',\n",
    "                            'dy_pred_N_FS',\n",
    "                            'dy_pred_S_FS',\n",
    "                            'dy_pred_N_FS_LGBM',\n",
    "                            'dy_pred_S_FS_LGBM',\n",
    "                            'dy_pred_N_FS_0808',\n",
    "                            'dy_pred_S_FS_0808',\n",
    "                            'dy_pred_N_FS_LGBM_0808',\n",
    "                            'dy_pred_S_FS_LGBM_0808']]\\\n",
    "                    .sort_values(by='dy_pred_N_FS', ascending=False)\\\n",
    "                    .drop_duplicates(subset=['Papel_root'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recommend.sort_values(by='dy_pred_N_FS_LGBM', ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_results = {'dy_pred_N_FS_0808'      :get_dy_metrics(df_recommend, 'dy_pred_N_FS_0808', 'besst_1', 1),\n",
    "                'dy_pred_S_FS_0808'      :get_dy_metrics(df_recommend, 'dy_pred_S_FS_0808', 'besst_1', 1),\n",
    "                'dy_pred_N_FS_LGBM_0808' :get_dy_metrics(df_recommend, 'dy_pred_N_FS_LGBM_0808', 'besst_1', 1),\n",
    "                'dy_pred_S_FS_LGBM_0808' :get_dy_metrics(df_recommend, 'dy_pred_S_FS_LGBM_0808', 'besst_1', 1),\n",
    "                'dy_pred_N_FS'      :get_dy_metrics(df_recommend, 'dy_pred_N_FS', 'besst_1', 1),\n",
    "                'dy_pred_S_FS'      :get_dy_metrics(df_recommend, 'dy_pred_S_FS', 'besst_1', 1),\n",
    "                'dy_pred_N_FS_LGBM' :get_dy_metrics(df_recommend, 'dy_pred_N_FS_LGBM', 'besst_1', 1),\n",
    "                'dy_pred_S_FS_LGBM' :get_dy_metrics(df_recommend, 'dy_pred_S_FS_LGBM', 'besst_1', 1)}\n",
    "\n",
    "dict_results2= {'dy_pred_N_FS_0808'      :get_dy_metrics(df_recommend, 'dy_pred_N_FS_0808', 'besst_2', 1),\n",
    "                'dy_pred_S_FS_0808'      :get_dy_metrics(df_recommend, 'dy_pred_S_FS_0808', 'besst_2', 1),\n",
    "                'dy_pred_N_FS_LGBM_0808' :get_dy_metrics(df_recommend, 'dy_pred_N_FS_LGBM_0808', 'besst_2', 1),\n",
    "                'dy_pred_S_FS_LGBM_0808' :get_dy_metrics(df_recommend, 'dy_pred_S_FS_LGBM_0808', 'besst_2', 1),\n",
    "                'dy_pred_N_FS'      :get_dy_metrics(df_recommend, 'dy_pred_N_FS', 'besst_2', 1),\n",
    "                'dy_pred_S_FS'      :get_dy_metrics(df_recommend, 'dy_pred_S_FS', 'besst_2', 1),\n",
    "                'dy_pred_N_FS_LGBM' :get_dy_metrics(df_recommend, 'dy_pred_N_FS_LGBM', 'besst_2', 1),\n",
    "                'dy_pred_S_FS_LGBM' :get_dy_metrics(df_recommend, 'dy_pred_S_FS_LGBM', 'besst_2', 1)}\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recommend['score'] = np.log(1 + df_recommend['dy_pred_N_FS_0808'].apply(lambda x: x if x>= 0 else 0)*dict_results['dy_pred_N_FS_0808']['top10'].loc['dy_mean']) + \\\n",
    "                        np.log(1 + df_recommend['dy_pred_S_FS_0808'].apply(lambda x: x if x>= 0 else 0)*dict_results['dy_pred_S_FS_0808']['top10'].loc['dy_mean']) + \\\n",
    "                        np.log(1 + df_recommend['dy_pred_N_FS_LGBM_0808'].apply(lambda x: x if x>= 0 else 0)*dict_results['dy_pred_N_FS_LGBM_0808']['top10'].loc['dy_mean']) + \\\n",
    "                        np.log(1 + df_recommend['dy_pred_S_FS_LGBM_0808'].apply(lambda x: x if x>= 0 else 0)*dict_results['dy_pred_S_FS_LGBM_0808']['top10'].loc['dy_mean']) + \\\n",
    "                        np.log(1 + df_recommend['dy_pred_N_FS'].apply(lambda x: x if x>= 0 else 0)*dict_results['dy_pred_N_FS']['top10'].loc['dy_mean']) + \\\n",
    "                        np.log(1 + df_recommend['dy_pred_S_FS'].apply(lambda x: x if x>= 0 else 0)*dict_results['dy_pred_S_FS']['top10'].loc['dy_mean']) + \\\n",
    "                        np.log(1 + df_recommend['dy_pred_N_FS_LGBM'].apply(lambda x: x if x>= 0 else 0)*dict_results['dy_pred_N_FS_LGBM']['top10'].loc['dy_mean']) + \\\n",
    "                        np.log(1 + df_recommend['dy_pred_S_FS_LGBM'].apply(lambda x: x if x>= 0 else 0)*dict_results['dy_pred_S_FS_LGBM']['top10'].loc['dy_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_results['score'] = get_dy_metrics(df_recommend, 'score', 'besst_1', 1)\n",
    "dict_results2['score'] = get_dy_metrics(df_recommend, 'score', 'besst_2', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_results = {}\n",
    "for key in dict_results.keys():\n",
    "    top_results[key] = df_recommend.sort_values(by=key, ascending=False)['dy_label'].iloc[:50].to_list()\n",
    "\n",
    "for key in dict_results.keys():\n",
    "    top_results[f'{key}_besst_1'] = df_recommend[df_recommend['besst_1']==1].sort_values(by=key, ascending=False)['dy_label'].iloc[:50].to_list()\n",
    "\n",
    "for key in dict_results.keys():\n",
    "    top_results[f'{key}_besst_2'] = df_recommend[df_recommend['besst_2']==1].sort_values(by=key, ascending=False)['dy_label'].iloc[:50].to_list()\n",
    "\n",
    "df_top_results = pd.DataFrame(top_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_results_melt_top20 = pd.melt(df_top_results.iloc[:20][df_top_results.columns], \n",
    "                                value_name='DY_mean_top20', var_name='Model').groupby('Model').mean()\n",
    "df_top_results_melt_top10 = pd.melt(df_top_results.iloc[:10][df_top_results.columns], \n",
    "                                value_name='DY_mean_top10', var_name='Model').groupby('Model').mean()\n",
    "df_top_results_melt_top5 = pd.melt(df_top_results.iloc[:5][df_top_results.columns], \n",
    "                                value_name='DY_mean_top5', var_name='Model').groupby('Model').mean()               \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_top_results_melt_full = pd.concat([df_top_results_melt_top5, df_top_results_melt_top10, df_top_results_melt_top20], axis=1)\n",
    "\n",
    "df_top_results_melt_top20 = pd.melt(df_top_results.iloc[:20][df_top_results.columns], \n",
    "                                value_name='DY_std_top20', var_name='Model').groupby('Model').std()\n",
    "df_top_results_melt_top10 = pd.melt(df_top_results.iloc[:10][df_top_results.columns], \n",
    "                                value_name='DY_std_top10', var_name='Model').groupby('Model').std()\n",
    "df_top_results_melt_top5 = pd.melt(df_top_results.iloc[:5][df_top_results.columns], \n",
    "                                value_name='DY_std_top5', var_name='Model').groupby('Model').std()  \n",
    "\n",
    "df_top_results_melt_full = pd.concat([df_top_results_melt_full,df_top_results_melt_top5, df_top_results_melt_top10, df_top_results_melt_top20], axis=1)\n",
    "\n",
    "df_top_results_melt_top20 = pd.melt(df_top_results.iloc[:20][df_top_results.columns], \n",
    "                                value_name='DY_median_top20', var_name='Model').groupby('Model').median()\n",
    "df_top_results_melt_top10 = pd.melt(df_top_results.iloc[:10][df_top_results.columns], \n",
    "                                value_name='DY_median_top10', var_name='Model').groupby('Model').median()\n",
    "df_top_results_melt_top5 = pd.melt(df_top_results.iloc[:5][df_top_results.columns], \n",
    "                                value_name='DY_median_top5', var_name='Model').groupby('Model').median()  \n",
    "\n",
    "df_top_results_melt_full = pd.concat([df_top_results_melt_full,df_top_results_melt_top5, df_top_results_melt_top10, df_top_results_melt_top20], axis=1)\n",
    "\n",
    "df_top_results_melt_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis for the overall stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_results_melt = df_top_results_melt_full.loc[[index for index in df_top_results_melt_full.index if 'besst_' not in index]]\n",
    "\n",
    "df_top_results_melt[['DY_sharpe_ratio_top5','DY_sharpe_ratio_top10','DY_sharpe_ratio_top20']] = \\\n",
    "        np.divide(df_top_results_melt[[col for col in df_top_results_melt.columns if 'mean' in col]].values, \n",
    "                  df_top_results_melt[[col for col in df_top_results_melt.columns if 'std' in col]].values)\n",
    "\n",
    "df_top_results_melt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 5 Stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(df_top_results_melt, 'DY_mean_top5', 'DY Mean Value - Top 5','rocket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(df_top_results_melt, 'DY_std_top5', 'DY Std Value - Top 5','viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(df_top_results_melt, 'DY_mean_top10', 'DY Std Value - Top 10','rocket')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 Stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(df_top_results_melt, 'DY_mean_top10', 'DY Std Value - Top 10','rocket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(df_top_results_melt, 'DY_std_top10', 'DY Std Value - Top 10','viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 20 Stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(df_top_results_melt, 'DY_mean_top20', 'DY Mean Value - Top 20','rocket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(df_top_results_melt, 'DY_std_top20', 'DY Std Value - Top 20','viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_syled_sheet(df_top_results_melt, 'mean', 'DY_mean_top10', 'vlag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_syled_sheet(df_top_results_melt, 'sharpe_ratio', 'DY_sharpe_ratio_top10', 'vlag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis for the BESST_1 Stocks (Sectors: A, B C ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_results_melt = df_top_results_melt_full.loc[[index for index in df_top_results_melt_full.index if 'besst_1' in index]]\n",
    "\n",
    "df_top_results_melt[['DY_sharpe_ratio_top5','DY_sharpe_ratio_top10','DY_sharpe_ratio_top20']] = \\\n",
    "        np.divide(df_top_results_melt[[col for col in df_top_results_melt.columns if 'mean' in col]].values, \n",
    "                  df_top_results_melt[[col for col in df_top_results_melt.columns if 'std' in col]].values)\n",
    "\n",
    "df_top_results_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_syled_sheet(df_top_results_melt, 'mean', 'DY_mean_top10', 'vlag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_syled_sheet(df_top_results_melt, 'sharpe_ratio', 'DY_sharpe_ratio_top10', 'vlag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis for the BESST_2 Stocks (Sectors: A, B C ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_results_melt = df_top_results_melt_full.loc[[index for index in df_top_results_melt_full.index if 'besst_2' in index]]\n",
    "\n",
    "df_top_results_melt[['DY_sharpe_ratio_top5','DY_sharpe_ratio_top10','DY_sharpe_ratio_top20']] = \\\n",
    "        np.divide(df_top_results_melt[[col for col in df_top_results_melt.columns if 'mean' in col]].values, \n",
    "                  df_top_results_melt[[col for col in df_top_results_melt.columns if 'std' in col]].values)\n",
    "\n",
    "df_top_results_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_syled_sheet(df_top_results_melt, 'mean', 'DY_mean_top10', 'vlag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_syled_sheet(df_top_results_melt, 'sharpe_ratio', 'DY_sharpe_ratio_top10', 'vlag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis for the overall Stocks between 11 and 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_results_melt_top20 = pd.melt(df_top_results.iloc[10:30][df_top_results.columns], \n",
    "                                value_name='DY_mean_top20', var_name='Model').groupby('Model').mean()\n",
    "df_top_results_melt_top10 = pd.melt(df_top_results.iloc[10:20][df_top_results.columns], \n",
    "                                value_name='DY_mean_top10', var_name='Model').groupby('Model').mean()\n",
    "df_top_results_melt_top5 = pd.melt(df_top_results.iloc[10:15][df_top_results.columns], \n",
    "                                value_name='DY_mean_top5', var_name='Model').groupby('Model').mean()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_results_melt_full = pd.concat([df_top_results_melt_top5, df_top_results_melt_top10, df_top_results_melt_top20], axis=1)\n",
    "\n",
    "df_top_results_melt_top20 = pd.melt(df_top_results.iloc[10:30][df_top_results.columns], \n",
    "                                value_name='DY_std_top20', var_name='Model').groupby('Model').std()\n",
    "df_top_results_melt_top10 = pd.melt(df_top_results.iloc[10:20][df_top_results.columns], \n",
    "                                value_name='DY_std_top10', var_name='Model').groupby('Model').std()\n",
    "df_top_results_melt_top5 = pd.melt(df_top_results.iloc[10:15][df_top_results.columns], \n",
    "                                value_name='DY_std_top5', var_name='Model').groupby('Model').std()  \n",
    "\n",
    "df_top_results_melt_full = pd.concat([df_top_results_melt_full,df_top_results_melt_top5, df_top_results_melt_top10, df_top_results_melt_top20], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_results_melt = df_top_results_melt_full.loc[[index for index in df_top_results_melt_full.index if 'besst_' not in index]]\n",
    "\n",
    "df_top_results_melt[['DY_sharpe_ratio_top5','DY_sharpe_ratio_top10','DY_sharpe_ratio_top20']] = \\\n",
    "        np.divide(df_top_results_melt[[col for col in df_top_results_melt.columns if 'mean' in col]].values, \n",
    "                  df_top_results_melt[[col for col in df_top_results_melt.columns if 'std' in col]].values)\n",
    "\n",
    "df_top_results_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(df_top_results_melt, 'DY_mean_top10', 'DY Mean Value - Top 10','rocket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(df_top_results_melt, 'DY_std_top10', 'DY Std Value - Top 10','viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_syled_sheet(df_top_results_melt, 'mean', 'DY_mean_top10', 'vlag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_syled_sheet(df_top_results_melt, 'sharpe_ratio', 'DY_sharpe_ratio_top10', 'vlag')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
