{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "This notebook was created to prepare the training and test datasets for this product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from scripts.utils import initialize_bucket, plot_importance\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import KFold, cross_val_score,cross_validate\n",
    "from sklearn.metrics import r2_score, mean_squared_error, make_scorer, explained_variance_score,mean_absolute_percentage_error\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import shap\n",
    "\n",
    "import pickle\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataframe(df, setor_encoding):\n",
    "    \"\"\"\n",
    "    Apply various transformations to the given DataFrame.\n",
    "\n",
    "    This function applies a series of calculations and replacements to columns\n",
    "    in the provided DataFrame to derive new values based on existing columns.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame containing necessary columns.\n",
    "        setor_encoding (dict): A dictionary mapping sector names to encoded values.\n",
    "\n",
    "    Returns:\n",
    "        None. The input DataFrame is modified in place with added/replaced columns.\n",
    "\n",
    "    Example:\n",
    "        >>> sector_encoding = {'Tech': 1, 'Finance': 2, 'Healthcare': 3}\n",
    "        >>> transform_dataframe(input_df, sector_encoding)\n",
    "        # The input_df DataFrame will be modified with new/replaced columns.\n",
    "    \"\"\"\n",
    "    # Replace values in 'dy_mean_std' column\n",
    "    df['dy_mean_std'][(df['dy_mean_std'] <= 0.005) & (df['dy_median_min'] <= 0.005)] = df['dy_mean_std'].max()\n",
    "\n",
    "    # Encode sectors using the provided setor_encoding dictionary\n",
    "    df['encoded_sectors'] = df['Setor'].replace(setor_encoding)\n",
    "\n",
    "    # Calculate power_valuation column\n",
    "    df['power_valuation'] = df['roe_mean_last'] * df['roic_mean_last'] * df['roe_mean_mean'] * df['roe_mean_mean']\n",
    "\n",
    "    # Calculate power_price column using multiple columns\n",
    "    df['power_price'] = (df['pl_mean_last'] * df['pa_mean_last']) / (df['pl_mean_ref_mean'] * df['pa_mean_ref_mean'])\n",
    "\n",
    "    # Calculate power_margin column\n",
    "    df['power_margin'] = (df['mrgliq_mean_last'] / 1_000_000) * (df['mrgebit_mean_last'] / 1_000_000)\n",
    "\n",
    "    # Calculate power_buy_flag column\n",
    "    df['power_buy_flag'] = df['pl_mean_last'] * df['dy_median_last'] * df['dy_median_min']\n",
    "\n",
    "    # Calculate power_stabil column\n",
    "    df['power_stabil'] = df['dy_median_min'] / df['dy_mean_std']\n",
    "\n",
    "    # Calculate power_stabil_delta column\n",
    "    df['power_stabil_delta'] = (df['dy_median_max'] - df['dy_median_min']) / df['dy_mean_std']\n",
    "\n",
    "    # Calculate power_min column\n",
    "    df['power_min'] = df['dy_median_min'] * df['dy_median_ref_min']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trends(data, roots):\n",
    "    \"\"\"\n",
    "    Calculate trends based on specified roots and add them to the data.\n",
    "\n",
    "    This function calculates trends for each specified root by dividing the last value\n",
    "    by the mean value and converting the result to a percentage change. The calculated\n",
    "    trends are then added as new columns to the provided data.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The input DataFrame containing necessary columns.\n",
    "        roots (list): A list of root names for which trends should be calculated.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A modified DataFrame with added trend columns.\n",
    "\n",
    "    Example:\n",
    "        >>> roots_to_calculate = ['roe', 'roic', 'pl']\n",
    "        >>> modified_df = create_trends(input_df, roots_to_calculate)\n",
    "        # The modified_df DataFrame will contain additional columns for trends.\n",
    "    \"\"\"\n",
    "    data_out = data.copy()\n",
    "    for root in roots:\n",
    "        try:\n",
    "            trend_column_name = root + '_trend'\n",
    "            trend_values = ((np.divide(data_out[root + '_last'], data_out[root + '_mean']) - 1) * 100)\n",
    "            data_out[trend_column_name] = trend_values\n",
    "        except:\n",
    "            continue\n",
    "    return data_out\n",
    "\n",
    "# Assuming 'df' is your DataFrame and 'roots' is a list of root names\n",
    "df = create_trends(df, roots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dummy(data, features):\n",
    "    \"\"\"\n",
    "    Transform categorical values into numerical values using one-hot encoding.\n",
    "    \n",
    "    Parameters:\n",
    "        data (DataFrame): The DataFrame containing the categorical features to be transformed.\n",
    "        features (list): List of categorical features to be transformed.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: The transformed DataFrame with numerical values for categorical features.\n",
    "    \"\"\"\n",
    "    for feature in features:\n",
    "        # Generate one-hot encoded columns and append to the DataFrame\n",
    "        one_hot_encoded = pd.get_dummies(data[feature])\n",
    "        new_columns = [(str(feature) + '_' + str(col)) for col in one_hot_encoded.columns]\n",
    "        data[new_columns] = one_hot_encoded\n",
    "        \n",
    "        # Drop the original categorical feature from the DataFrame\n",
    "        data.drop(feature, axis=1, inplace=True)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_name_cleaner(col):\n",
    "    \"\"\"\n",
    "    Clean column names by replacing non-UTF-8 characters with a replacement character.\n",
    "\n",
    "    Parameters:\n",
    "        col (str): The column name to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned column name.\n",
    "    \"\"\"\n",
    "    return col.encode('utf-8', 'replace').decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_year_month_column(data: pd.DataFrame, year_column: str, month_column: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a new column 'year_month' in the DataFrame by concatenating values from two columns.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The DataFrame containing the columns 'year_last' and 'month_last'.\n",
    "        year_column (str): Name of the column containing the year values.\n",
    "        month_column (str): Name of the column containing the month values.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with a new 'year_month' column.\n",
    "    \"\"\"\n",
    "    # Ensure that the specified columns exist in the DataFrame\n",
    "    if year_column not in data.columns or month_column not in data.columns:\n",
    "        raise ValueError(f\"Columns '{year_column}' and '{month_column}' not found in the DataFrame.\")\n",
    "    \n",
    "    # Create the 'year_month' column by concatenating the values from 'year_last' and 'month_last' columns\n",
    "    data['year_month'] = data[year_column].astype(str) + data[month_column].astype(str)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 400)\n",
    "\n",
    "credentials_path = '../datascience-capstone-project-05b1642f45c3.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client, bucket = initialize_bucket(credentials_path,'storage-barsianize')\n",
    "\n",
    "path = \"gs://storage-barsianize/03_refined/df_windowed_full.parquet\"\n",
    "base_dataset =  pd.read_parquet(path)\n",
    "\n",
    "df = base_dataset.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "                'Empresa', 'Papel',\n",
    "                'year_last', 'month_last',\n",
    "                'besst_1', 'besst_2',\n",
    "                'Setor','Tipo',\n",
    "                'Lucro_Liquido_12m_category',\n",
    "                # 'Valor_de_mercado_category',\n",
    "                # 'Patrim_Liq_category',\n",
    "                'dy_label',\n",
    "                'dy_median_last', 'dy_median_max','dy_median_min','dy_spread','dy_mean_std','dy_mean_last', 'dy_median_ref_min',#add dy relativo\n",
    "                'euro_last','euro_mean', #add euro relativo\n",
    "                'evebit_mean_ref_mean','evebit_mean_last','evebit_mean_mean', #add evebit relativo\n",
    "                'c5y_mean_last','c5y_mean_mean', #add c5y relativo\n",
    "                'cotacao_max_ref_max','cotacao_mean_ref_mean','cotacao_mean_ref_std', 'cotacao_mean_last', 'cotacao_mean_mean',#add cotacao relativo\n",
    "                'divbpatr_max_ref_max','divbpatr_mean_last','divbpatr_mean_mean', #add divbpatr relativo\n",
    "                'dolar_comercial_last','dolar_comercial_mean', #add euro relativo\n",
    "                'ibovespa_spread','ibovespa_spread_relevance','ibovespa_last','ibovespa_mean',#add ibovespa relativo\n",
    "                'pib_dolarizado_last','pib_dolarizado_mean', #add pib_dolarizado relativo\n",
    "                'preco_do_petroleo_last','preco_do_petroleo_mean', #add preco_do_petroleo relativo\n",
    "                'igpm_last','igpm_mean',#add igpm relativo\n",
    "                'ipca_last','ipca_mean',#add ipca relativo\n",
    "                'selic_last','selic_mean',#add igpm relativo\n",
    "                'liq2m_mean_ref_mean','liq2m_mean_last','liq2m_mean_mean', #add liq2m relativo\n",
    "                'liqc_mean_ref_mean','liqc_mean_last','liqc_mean_mean', #add liqc relativo\n",
    "                'mrgebit_mean_ref_mean','mrgebit_mean_last','mrgebit_mean_mean', #add mrgebit relativo\n",
    "                'mrgliq_mean_ref_mean','mrgliq_mean_last','mrgliq_mean_mean', #add mrgliq relativo\n",
    "                'pa_mean_ref_mean','pa_mean_last', 'pa_mean_mean',#add pa relativo\n",
    "                'pl_mean_ref_mean','pl_mean_last', 'pl_mean_mean',#add pa relativo\n",
    "                'pcg_mean_ref_mean','pcg_mean_last', 'pcg_mean_mean',#add pcg relativo\n",
    "                'pebit_mean_ref_mean','pebit_mean_last', 'pebit_mean_mean',#add pebit relativo\n",
    "                'pacl_mean_ref_mean','pacl_mean_last', 'pacl_mean_mean',#add pacl relativo\n",
    "                'psr_mean_ref_mean','psr_mean_last', 'psr_mean_mean',#add psr relativo\n",
    "                'pvp_mean_ref_mean','pvp_mean_last', 'pvp_mean_mean',#add pvp relativo\n",
    "                'roe_mean_ref_mean','roe_mean_last', 'roe_mean_mean',#add roe relativo\n",
    "                'roic_mean_ref_mean','roic_mean_last', 'roic_mean_mean',#add roic relativo\n",
    "                'patrliq_mean_ref_mean','patrliq_mean_last','patrliq_mean_mean', #add patrliq relativo\n",
    "            ]\n",
    "\n",
    "roots = [\n",
    "            'euro',\n",
    "            'dy_mean',\n",
    "            'evebit_mean',\n",
    "            'c5y_mean',\n",
    "            'cotacao_mean',\n",
    "            'divbpatr_mean',\n",
    "            'dolar_comercial',\n",
    "            'ibovespa',\n",
    "            'pib_dolarizado',\n",
    "            'preco_do_petroleo',\n",
    "            'igpm',\n",
    "            'ipca',\n",
    "            'selic',\n",
    "            'liq2m',\n",
    "            'liqc',\n",
    "            'mrgebit',\n",
    "            'mrgliq',\n",
    "            'pa_mean',\n",
    "            'pl_mean',\n",
    "            'pcg_mean',\n",
    "            'pebit_mean',\n",
    "            'pacl_mean',\n",
    "            'psr_mean',\n",
    "            'pvp_mean',\n",
    "            'roe_mean',\n",
    "            'roic_mean_last',\n",
    "            'patrliq_mean'\n",
    "        ]\n",
    "\n",
    "setor_encoding = {\n",
    "    'Computadores e Equipamentos':1,\n",
    "    'Tecidos, Vestuário e Calçados':2,\n",
    "    'Máquinas e Equipamentos':3,\n",
    "    'Equipamentos':4,\n",
    "    'Automóveis e Motocicletas':5,\n",
    "    'Bebidas':6,\n",
    "    'Alimentos Processados':7,\n",
    "    'Produtos de Uso Pessoal e de Limpeza':8,\n",
    "    'Utilidades Domésticas':9,\n",
    "    'Embalagens':10,\n",
    "    'Materiais Diversos':11,\n",
    "    'Medicamentos e Outros Produtos':12,\n",
    "    'Serv.Méd.Hospit. Análises e Diagnósticos':13,\n",
    "    'Hoteis e Restaurantes':14,\n",
    "    'Viagens e Lazer':15,\n",
    "    'Transporte':16,\n",
    "    'Material de Transporte':17,\n",
    "    'Serviços Diversos':18,\n",
    "    'Programas e Serviços':19,\n",
    "    'Comércio e Distribuição':20,\n",
    "    'Comércio':21,\n",
    "    'Telecomunicações':22,\n",
    "    'Mídia':23,\n",
    "    'Holdings Diversificadas':24,\n",
    "    'Serviços Financeiros Diversos':25,\n",
    "    'Intermediários Financeiros':26,\n",
    "    'Previdência e Seguros':27,\n",
    "    'Exploração de Imóveis':28,\n",
    "    'Químicos':29,\n",
    "    'Construção e Engenharia':30,\n",
    "    'Construção Civil':31,\n",
    "    'Siderurgia e Metalurgia':32,\n",
    "    'Energia Elétrica':33,\n",
    "    'Água e Saneamento':34,\n",
    "    'Gás':35,\n",
    "    'Petróleo, Gás e Biocombustíveis':36,\n",
    "    'Mineração':37,\n",
    "    'Madeira e Papel':38,\n",
    "    'Agropecuária':39,\n",
    "    'Diversos':40,\n",
    "    'Outros':41,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[features]\n",
    "\n",
    "df = transform_dataframe(df, setor_encoding)\n",
    "\n",
    "df.columns = [column_name_cleaner(col) for col in df.columns]\n",
    "\n",
    "df = create_year_month_column(df, 'year_last', 'month_last')\n",
    "\n",
    "df = transform_dummy(df,['Tipo'])\n",
    "\n",
    "object_cols = df.columns[df.dtypes == 'object'] \n",
    "print(object_cols)\n",
    "bool_cols = df.columns[df.dtypes == 'bool'] \n",
    "print(bool_cols)\n",
    "\n",
    "# dropping columns with maximum values larger than 1 trillion dollars\n",
    "to_drop = df.columns[df.dtypes=='float64'][(df[df.columns[df.dtypes=='float64']].max()>1_000_000_000_000)]\n",
    "print(df[to_drop].max())\n",
    "df = df.drop(to_drop, axis=1)\n",
    "\n",
    "# transforming bool_cols to float\n",
    "df[bool_cols] = df[bool_cols].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking NaN values and inputing median\n",
    "df.isna().mean().sort_values(ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inpute 0 for missing values\n",
    "df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = df.corr()['dy_label']\n",
    "\n",
    "training_cols = np.abs(cor).sort_values(ascending=False)[0:150].index\n",
    "\n",
    "plt.figure(figsize=(17,7))\n",
    "np.abs(cor).sort_values(ascending=False)[1:20].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_train = df[['Empresa', 'Papel', 'year_month'] + list(training_cols)].copy()\n",
    "\n",
    "df_to_pred  = df_to_train[df_to_train['year_month']=='2022.04.0'].copy()\n",
    "df = df_to_train[df_to_train['year_month']!='2022.04.0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client, bucket = initialize_bucket(credentials_path,'storage-barsianize')\n",
    "\n",
    "path_local = '../data/04_datasets/'\n",
    "filename = 'df_base_dataset.parquet'\n",
    "blob_name = '05_datasets/'\n",
    "\n",
    "# save the DataFrame as a parquet file\n",
    "df.to_parquet(path_local + filename)\n",
    "\n",
    "# upload the parquet file to Google Cloud Storage\n",
    "blob = bucket.blob(blob_name + filename)\n",
    "blob._chunk_size = 8388608\n",
    "blob.upload_from_filename(path_local + filename, num_retries=10, )\n",
    "\n",
    "\n",
    "\n",
    "client, bucket = initialize_bucket(credentials_path,'storage-barsianize')\n",
    "\n",
    "path_local = '../data/04_datasets/'\n",
    "filename = 'df_to_pred.parquet'\n",
    "blob_name = '05_datasets/'\n",
    "\n",
    "# save the DataFrame as a parquet file\n",
    "df.to_parquet(path_local + filename)\n",
    "\n",
    "# upload the parquet file to Google Cloud Storage\n",
    "blob = bucket.blob(blob_name + filename)\n",
    "blob._chunk_size = 8388608\n",
    "blob.upload_from_filename(path_local + filename, num_retries=10, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
