{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering\n",
    "\n",
    "This notebook is designed to extract raw fundamentalist data and refine it by cleaning, aggregating, completing and consolidating different external datasets.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import time\n",
    "\n",
    "import fundamentus\n",
    "\n",
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from scripts.utils import initialize_bucket\n",
    "import requests\n",
    "import requests_cache\n",
    "import logging\n",
    "import time\n",
    "from datetime import date\n",
    "\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 200)\n",
    "\n",
    "credentials_path = '../datascience-capstone-project-05b1642f45c3.json'\n",
    "\n",
    "client, bucket = initialize_bucket(credentials_path,'storage-barsianize')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perc_to_float(val):\n",
    "    \"\"\"\n",
    "    Percent to float\n",
    "      - replace string in pt-br to float\n",
    "      - from '45,56%' to 0.4556\n",
    "    Input:\n",
    "        (DataFrame, column_name)\n",
    "    \"\"\"\n",
    "\n",
    "    res = val\n",
    "    res = res.replace( to_replace=r'[%]', value='' , regex=True )\n",
    "    res = res.replace( to_replace=r'[.]', value='' , regex=True )\n",
    "    res = res.replace( to_replace=r'[,]', value='.', regex=True )\n",
    "    res = res.astype(float) / 100\n",
    "\n",
    "    return res\n",
    "\n",
    "def _rename_cols(data):\n",
    "    \"\"\"\n",
    "    Rename columns in DataFrame\n",
    "      - use a valid Python identifier\n",
    "      - so each column can be a DataFrame property\n",
    "      - Example:\n",
    "          df.pl > 0\n",
    "    \"\"\"\n",
    "\n",
    "    df2 = pd.DataFrame()\n",
    "\n",
    "    ## Fix: rename columns\n",
    "    df2['cotacao'  ] = data['Cotação'          ]\n",
    "    df2['pl'       ] = data['P/L'              ]\n",
    "    df2['pvp'      ] = data['P/VP'             ]\n",
    "    df2['psr'      ] = data['PSR'              ]\n",
    "    df2['dy'       ] = data['Div.Yield'        ]\n",
    "    df2['pa'       ] = data['P/Ativo'          ]\n",
    "    df2['pcg'      ] = data['P/Cap.Giro'       ]\n",
    "    df2['pebit'    ] = data['P/EBIT'           ]\n",
    "    df2['pacl'     ] = data['P/Ativ Circ.Liq'  ]\n",
    "    # df2['evebit'   ] = data['EV/EBIT'          ]\n",
    "    # df2['evebitda' ] = data['EV/EBITDA'        ] ##\n",
    "    df2['mrgebit'  ] = data['Mrg Ebit'         ]\n",
    "    df2['mrgliq'   ] = data['Mrg. Líq.'        ]\n",
    "    df2['roic'     ] = data['ROIC'             ]\n",
    "    df2['roe'      ] = data['ROE'              ]\n",
    "    df2['liqc'     ] = data['Liq. Corr.'       ]\n",
    "    df2['liq2m'    ] = data['Liq.2meses'       ]\n",
    "    df2['patrliq'  ] = data['Patrim. Líq'      ]\n",
    "    df2['divbpatr' ] = data['Dív.Brut/ Patrim.']\n",
    "    df2['c5y'      ] = data['Cresc. Rec.5a'    ]\n",
    "\n",
    "    return df2\n",
    "\n",
    "def get_resultado_raw(url):\n",
    "    \"\"\"\n",
    "    Get data from fundamentus:\n",
    "      URL:\n",
    "        http://fundamentus.com.br/resultado.php\n",
    "    RAW:\n",
    "      DataFrame preserves original HTML header names\n",
    "    Output:\n",
    "      DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    ##\n",
    "    ## Busca avançada por empresa\n",
    "    ##\n",
    "    # url = 'http://www.fundamentus.com.br/resultado.php'\n",
    "    hdr = {'User-agent': 'Mozilla/5.0 (Windows; U; Windows NT 6.1; rv:2.2) Gecko/20110201',\n",
    "           'Accept': 'text/html, text/plain, text/css, text/sgml, */*;q=0.01',\n",
    "           'Accept-Encoding': 'gzip, deflate',\n",
    "           }\n",
    "\n",
    "    with requests_cache.enabled():\n",
    "        content = requests.get(url, headers=hdr)\n",
    "\n",
    "        if content.from_cache:\n",
    "            logging.debug('.../resultado.php: [CACHED]')\n",
    "        else: # pragma: no cover\n",
    "            logging.debug('.../resultado.php: sleeping...')\n",
    "            time.sleep(.500) # 500 ms\n",
    "\n",
    "\n",
    "    ## parse + load\n",
    "    df = pd.read_html(content.text, decimal=\",\", thousands='.')[0]\n",
    "\n",
    "    ## Fix: percent string\n",
    "    df['Div.Yield']     = perc_to_float( df['Div.Yield']     )\n",
    "    df['Mrg Ebit']      = perc_to_float( df['Mrg Ebit']      )\n",
    "    df['Mrg. Líq.']     = perc_to_float( df['Mrg. Líq.']     )\n",
    "    df['ROIC']          = perc_to_float( df['ROIC']          )\n",
    "    df['ROE']           = perc_to_float( df['ROE']           )\n",
    "    df['Cresc. Rec.5a'] = perc_to_float( df['Cresc. Rec.5a'] )\n",
    "\n",
    "    ## index by 'Papel', instead of 'int'\n",
    "    df.index = df['Papel']\n",
    "    df.drop('Papel', axis='columns', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    ## naming\n",
    "    df.name = 'Fundamentus: HTML names'\n",
    "    df.columns.name = 'Multiples'\n",
    "    df.index.name = 'papel'\n",
    "\n",
    "    ## return sorted by 'papel'\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_resultado(url):\n",
    "    \"\"\"\n",
    "    Data from fundamentus, fixing header names.\n",
    "      URL:\n",
    "        given from the user\n",
    "      Obs:\n",
    "        DataFrame uses short header names\n",
    "    Output:\n",
    "      DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    ## get RAW data\n",
    "    data1 = get_resultado_raw(url)\n",
    "\n",
    "    ## rename!\n",
    "    data2 = _rename_cols(data1)\n",
    "\n",
    "    ## metadata\n",
    "    data2.name = 'Fundamentus: short names'\n",
    "    data2.columns.name = 'Multiples'\n",
    "    data2.index.name = 'papel'\n",
    "\n",
    "    ## remove duplicates\n",
    "#   df = data2.drop_duplicates(subset=['cotacao','pl','pvp'], keep='last')\n",
    "    df = data2.drop_duplicates(keep='first')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates(urls):\n",
    "    \"\"\"\n",
    "    Extracts dates from a list of URLs.\n",
    "    \n",
    "    Args:\n",
    "        urls (list): A list of URLs.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of datetime objects representing the extracted dates.\n",
    "    \"\"\"\n",
    "    dates = []\n",
    "    for value in urls:\n",
    "        str_1 = re.split('/web/', value)[1]\n",
    "        str_2 = re.split('/http', str_1)[0]\n",
    "        str_date = f'{str_2[:4]}-{str_2[4:6]}-{str_2[6:8]}'\n",
    "        dates.append(pd.to_datetime(str_date))\n",
    "    return dates\n",
    "\n",
    "\n",
    "\n",
    "def ingest_data(file: str, path: str, time_to_sleep: int = 1, store_locally: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ingests data from the specified file and path, processing the URLs and retrieving the resultado.\n",
    "    \n",
    "    Args:\n",
    "        file (str): The name of the file.\n",
    "        path (str): The path to the file.\n",
    "        time_to_sleep (int, optional): The duration to sleep between processing each URL. Defaults to 1.\n",
    "        store_locally (bool, optional): Whether to store the processed data locally. Defaults to False.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The ingested and processed data as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    filename = path + file\n",
    "    urls = pd.read_csv(filename).sort_values(by='urls')\n",
    "\n",
    "    # Extract dates from URLs\n",
    "    dates = get_dates(urls['urls'].values)\n",
    "\n",
    "    # Create a list of tuples (date, url)\n",
    "    date_url = list(zip(dates, urls['urls'].values))\n",
    "\n",
    "    # Initialize an empty DataFrame to store the results\n",
    "    df_full = pd.DataFrame()\n",
    "\n",
    "    # Get the year of the first date\n",
    "    year = dates[0].year\n",
    "\n",
    "    # Iterate over each date and URL\n",
    "    for date, url in date_url[:]:\n",
    "        print(date, url)\n",
    "\n",
    "        # Get the resultado for the current URL\n",
    "        df = get_resultado(url)\n",
    "\n",
    "        # Add date and year columns to the resultado DataFrame\n",
    "        df['date'] = date\n",
    "        df['year'] = date.year\n",
    "\n",
    "        # Concatenate the resultado DataFrame with the full DataFrame\n",
    "        df_full = pd.concat([df_full, df])\n",
    "\n",
    "        # Check if the year has changed\n",
    "        if year == date.year:\n",
    "            continue\n",
    "        elif store_locally:\n",
    "            # Store the data locally for the current year\n",
    "            df_full.loc[df_full['year'] == year].to_csv(f'../data/01_trusted/{str(date.year)}.csv')\n",
    "            year = date.year\n",
    "\n",
    "        # Sleep between processing URLs\n",
    "        time.sleep(time_to_sleep)\n",
    "\n",
    "    # Reset the index of the full DataFrame\n",
    "    return df_full.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "def get_detailed_ticker_data(tickers: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieves detailed ticker data for the given list of tickers.\n",
    "    \n",
    "    Args:\n",
    "        tickers (list): A list of tickers to retrieve data for.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The detailed ticker data as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    df_tickers = pd.DataFrame()\n",
    "\n",
    "    # Iterate over each ticker\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # Retrieve ticker data using fundamentus library\n",
    "            df = fundamentus.get_ticker(ticker)\n",
    "\n",
    "            # Concatenate ticker data with the existing DataFrame\n",
    "            df_tickers = pd.concat([df_tickers, df])\n",
    "        except:\n",
    "            # Print error message for failed ticker retrieval\n",
    "            print(f'Failed to retrieve ticker data for: {ticker}')\n",
    "\n",
    "        # Sleep for 3 seconds before retrieving the next ticker data\n",
    "        time.sleep(3)\n",
    "\n",
    "    return df_tickers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refining Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Daily Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_daily_dataset(data: pd.DataFrame, date_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a daily dataset by merging the given data on a daily frequency.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The original dataset.\n",
    "        date_col (str): The name of the column containing the dates.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The daily dataset.\n",
    "    \"\"\"\n",
    "    # Generate a range of daily dates from the minimum to the maximum date in the dataset\n",
    "    date_range = pd.date_range(data[date_col].min(), data[date_col].max(), freq='D')\n",
    "\n",
    "    # Create a DataFrame with 'days' column containing daily dates\n",
    "    data_daily = pd.DataFrame(date_range, columns=['days'])\n",
    "\n",
    "    # Merge the original data with the daily DataFrame based on the 'days' column and date_col\n",
    "    data_daily = data_daily.merge(data, how='left', left_on='days', right_on=date_col)\n",
    "\n",
    "    return data_daily\n",
    "\n",
    "\n",
    "def interpolate_data(data: pd.DataFrame, cols_first: list, cols_spline: list, date_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Interpolates missing values in the data based on the specified columns.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The input dataset.\n",
    "        cols_first (list): The columns to be filled with the first non-null value.\n",
    "        cols_spline (list): The columns to be interpolated using spline interpolation.\n",
    "        date_col (str): The name of the column containing the dates.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The dataset with interpolated values.\n",
    "    \"\"\"\n",
    "    # Create a daily dataset with missing dates\n",
    "    data = create_daily_dataset(data, date_col)\n",
    "\n",
    "    columns = data.columns\n",
    "\n",
    "    # Iterate over each column\n",
    "    for col in columns:\n",
    "        if col in cols_first:\n",
    "            # Fill missing values in cols_first with the first non-null value\n",
    "            data[col] = data[col].fillna(data[col][0])\n",
    "        elif col in cols_spline:\n",
    "            # Interpolate missing values in cols_spline using spline interpolation\n",
    "            data[col] = data[col].interpolate(method='spline', order=2)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_daily_data_per_ticker(data: pd.DataFrame, ticker_col: str, cols_first: list, cols_spline: list, date_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieves daily data per ticker by interpolating missing values in the specified columns.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The input dataset.\n",
    "        ticker_col (str): The name of the column containing the tickers.\n",
    "        cols_first (list): The columns to be filled with the first non-null value.\n",
    "        cols_spline (list): The columns to be interpolated using spline interpolation.\n",
    "        date_col (str): The name of the column containing the dates.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The daily data per ticker with interpolated values.\n",
    "    \"\"\"\n",
    "    # Get unique tickers from the dataset\n",
    "    tickers = data[ticker_col].unique()\n",
    "\n",
    "    # Create a list to store daily dataframes per ticker\n",
    "    daily_dataframes = []\n",
    "\n",
    "    # Iterate over each ticker\n",
    "    for ticker in tickers:\n",
    "        print(ticker)\n",
    "        try:\n",
    "            # Interpolate missing values for the specific ticker\n",
    "            daily_dataframes.append(\n",
    "                interpolate_data(data[data[ticker_col] == ticker], cols_first, cols_spline, date_col)\n",
    "            )\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Concatenate daily dataframes for all tickers\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "\n",
    "    return daily_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering and data cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_numerical_categories(data: pd.DataFrame, cols: list, n: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates numerical categories for the specified columns based on quantiles.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The input dataset.\n",
    "        cols (list): The columns for which numerical categories will be created.\n",
    "        n (int): The number of quantiles to use for categorization.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The dataset with numerical categories.\n",
    "    \"\"\"\n",
    "    # Iterate over each column\n",
    "    for col in cols:\n",
    "        # Create a new column for numerical categories\n",
    "        data[col + '_category'] = pd.qcut(data[col], q=n, duplicates='drop', labels=list(range(n)))\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def create_besst_categories(data: pd.DataFrame, besst_col: str, besst_1: list, besst_2: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates binary categories based on the values in the specified column.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The input dataset.\n",
    "        besst_col (str): The name of the column used for categorization.\n",
    "        besst_1 (list): Values considered as category 1.\n",
    "        besst_2 (list): Values considered as category 2.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The dataset with binary categories.\n",
    "    \"\"\"\n",
    "    # Create a new column to indicate if the values are in besst_1\n",
    "    data['besst_1'] = np.isin(data[besst_col], besst_1)\n",
    "\n",
    "    # Create a new column to indicate if the values are in besst_2\n",
    "    data['besst_2'] = np.isin(data[besst_col], besst_2)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def correct_data_column(x: str) -> str:\n",
    "    \"\"\"\n",
    "    Corrects the format of a date string from 'DD/MM/YYYY' to 'YYYY-MM-DD'.\n",
    "    \n",
    "    Args:\n",
    "        x (str): The date string in 'DD/MM/YYYY' format.\n",
    "        \n",
    "    Returns:\n",
    "        str: The date string in 'YYYY-MM-DD' format.\n",
    "    \"\"\"\n",
    "    # Split the date string into day, month, and year\n",
    "    date = x.split('/')\n",
    "\n",
    "    # Rearrange the date components and join them with '-' separator\n",
    "    new_date = '-'.join(date[::-1])\n",
    "\n",
    "    return new_date\n",
    "\n",
    "def from_percent_to_numeric(x):\n",
    "    \"\"\"\n",
    "    Convert a percentage string to a numeric value.\n",
    "\n",
    "    This function takes a string representing a percentage, removes the '%' character,\n",
    "    and converts the resulting string to a floating-point numeric value by dividing it by 100.\n",
    "\n",
    "    Args:\n",
    "        x (str): A string representing a percentage, e.g., '25.5%'.\n",
    "\n",
    "    Returns:\n",
    "        float: A numeric value corresponding to the input percentage after conversion.\n",
    "\n",
    "    Example:\n",
    "        >>> from_percent_to_numeric('25.5%')\n",
    "        0.255\n",
    "    \"\"\"\n",
    "    numeric_value = np.float(re.sub('%', '', x)) / 100\n",
    "    return numeric_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregator_monthly(data: pd.DataFrame, tickers: list, ticker_col: str, date_col: str, agg_dict: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregates data on a monthly basis for the specified tickers using the provided aggregation dictionary.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The input dataset.\n",
    "        tickers (list): The list of tickers to aggregate.\n",
    "        ticker_col (str): The name of the column containing the tickers.\n",
    "        date_col (str): The name of the column containing the dates.\n",
    "        agg_dict (dict): The dictionary specifying the aggregations to be performed.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The aggregated data on a monthly basis.\n",
    "    \"\"\"\n",
    "    # Create an empty list to store the monthly aggregated dataframes\n",
    "    df_agg_monthly_full = []\n",
    "\n",
    "    # Iterate over each ticker\n",
    "    for ticker in tickers:\n",
    "        # Filter the data for the current ticker and perform monthly aggregation\n",
    "        df_agg_monthly = data.loc[data[ticker_col] == ticker].resample('M', on=date_col).agg(agg_dict)\n",
    "\n",
    "        # Append the monthly aggregated dataframe to the list\n",
    "        df_agg_monthly_full.append(df_agg_monthly)\n",
    "\n",
    "    # Concatenate all the monthly aggregated dataframes\n",
    "    df_agg_monthly_full = pd.concat(df_agg_monthly_full).reset_index(drop=True)\n",
    "\n",
    "    # Modify column names to include the original column name and aggregation function (except for 'first' function)\n",
    "    df_agg_monthly_full.columns = [x[0] + '_' + x[1] if 'first' != x[1] else x[0] for x in df_agg_monthly_full.columns]\n",
    "\n",
    "    return df_agg_monthly_full\n",
    "\n",
    "\n",
    "def get_windowed_data(data: pd.DataFrame, agg_dict: dict, tickers: list, ticker_col: str, columns_to_first: list,\n",
    "                      columns_to_last: list, agg_dict_ref: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies windowed aggregations and shifts to the data for each ticker.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The input dataset.\n",
    "        agg_dict (dict): The dictionary specifying the aggregations to be performed.\n",
    "        tickers (list): The list of tickers.\n",
    "        ticker_col (str): The name of the column containing the tickers.\n",
    "        columns_to_first (list): The list of columns to shift using the 'first' value.\n",
    "        columns_to_last (list): The list of columns to shift using the 'last' value.\n",
    "        agg_dict_ref (dict): The dictionary specifying the aggregations for the reference window.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The windowed data with aggregations and shifted values.\n",
    "    \"\"\"\n",
    "    # Create an empty list to store the windowed dataframes\n",
    "    data_windowed_full = []\n",
    "\n",
    "    # Iterate over each ticker\n",
    "    for ticker in tickers:\n",
    "        # Filter the data for the current ticker\n",
    "        data_ticker = data[data[ticker_col] == ticker]\n",
    "\n",
    "        # Apply rolling aggregations to the data\n",
    "        data_windowed = data_ticker.rolling(12).agg(agg_dict)\n",
    "        data_windowed_ref = data_ticker.rolling(12).agg(agg_dict_ref)\n",
    "\n",
    "        # Concatenate the rolling aggregations and reference aggregations\n",
    "        data_windowed = pd.concat([data_windowed, data_windowed_ref], axis=1)\n",
    "\n",
    "        # Modify column names to include the original column name and aggregation function (except for 'first' function)\n",
    "        column_suffix = lambda x: x[0] + '_' + x[1] if x[1] != 'first' else x[0]\n",
    "        data_windowed.columns = [column_suffix(x) for x in data_windowed.columns]\n",
    "\n",
    "        # Shift selected columns using 'first' and 'last' values\n",
    "        data_windowed[columns_to_first] = data_ticker[columns_to_first].shift(12)\n",
    "        data_windowed[[col + '_last' for col in columns_to_last]] = data_ticker[columns_to_last].shift(12)\n",
    "\n",
    "        # Shift the 'dy_last' column and drop rows with NaN values in 'dy_label'\n",
    "        data_windowed['dy_label'] = data_ticker['dy_last'].shift(24)\n",
    "        data_windowed.dropna(subset=['dy_label'], inplace=True)\n",
    "\n",
    "        # Append the windowed dataframe to the list\n",
    "        data_windowed_full.append(data_windowed)\n",
    "\n",
    "    # Concatenate all the windowed dataframes\n",
    "    data_windowed_full = pd.concat(data_windowed_full)\n",
    "\n",
    "    # Fill NaN values with backward filling method\n",
    "    data_windowed_full.fillna(method='bfill', inplace=True)\n",
    "\n",
    "    return data_windowed_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting External Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bcb_data(codes, start_date, end_date, output_format='json'):\n",
    "    \"\"\"\n",
    "    Get macroeconomic data from the Brazilian Central Bank's API.\n",
    "\n",
    "    Args:\n",
    "        codes (list): List of series codes to retrieve. See the API documentation for available codes.\n",
    "        start_date (str): Start date in format 'dd/mm/yyyy'.\n",
    "        end_date (str): End date in format 'dd/mm/yyyy'.\n",
    "        output_format (str, optional): Output format, either 'json' or 'csv'. Defaults to 'json'.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with the requested series data.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the output format is invalid.\n",
    "        requests.exceptions.RequestException: If the API request fails.\n",
    "\n",
    "    Example:\n",
    "        Get the values of the crude oil production, commodities, dollar, euro, IPCA, IGPM, and Selic series from January 1st, 2021 to December 31st, 2021 in JSON format:\n",
    "\n",
    "        >>> codes = [13522, 13521, 4390, 189, 11, 1178]\n",
    "        >>> start_date = '01/01/2021'\n",
    "        >>> end_date = '31/12/2021'\n",
    "        >>> output_format = 'json'\n",
    "        >>> df = get_bcb_data(codes, start_date, end_date, output_format)\n",
    "\n",
    "    \"\"\"\n",
    "    # Define the base URL of the API\n",
    "    url_base = \"https://api.bcb.gov.br/dados/serie/bcdata.sgs.{}/dados\"\n",
    "\n",
    "    # Define the output format parameter\n",
    "    if output_format not in ['json', 'csv']:\n",
    "        raise ValueError(\"Invalid output format. Must be 'json' or 'csv'.\")\n",
    "    formato = output_format\n",
    "\n",
    "    # Create a dictionary to store the series DataFrames\n",
    "    dataframes = {}\n",
    "\n",
    "    # Get the data for each series and store it in a DataFrame\n",
    "    for code_name, code in codes.items():\n",
    "        # Build the complete URL with the defined parameters\n",
    "        url = url_base.format(code) + f\"?formato={formato}&dataInicial={start_date}&dataFinal={end_date}\"\n",
    "        # Make the API request\n",
    "        response = requests.get(url)\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Convert the response to a pandas DataFrame and set the column name to the series code\n",
    "            try:\n",
    "                df = pd.read_json(response.text)\n",
    "            except:\n",
    "                try:\n",
    "                    df = pd.read_xml(response.text)\n",
    "                except:\n",
    "                    continue\n",
    "            col_name = code_name\n",
    "            df = df.rename(columns={\"valor\": col_name})\n",
    "            # Set the DataFrame index to the date\n",
    "            df = df.set_index(\"data\")\n",
    "            # Store the DataFrame in the dictionary\n",
    "            dataframes[col_name] = df[col_name]\n",
    "        else:\n",
    "            raise requests.exceptions.RequestException(f\"Error getting data. HTTP status code: {response.status_code}\")\n",
    "\n",
    "    # Combine the DataFrames for each series into a single DataFrame\n",
    "    final_df = pd.concat(dataframes.values(), axis=1).reset_index()\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "### Data Collection\n",
    "\n",
    "For the data gathering, two repositories on Github were combined, so that the historical data could be scraped from the web. This Raw data is beeing stored in the cloud (GCP) and are the main data used for the project.\n",
    "\n",
    "Repositories utilized:\n",
    "* https://github.com/mv/fundamentus-api\n",
    "* https://github.com/Victorcorcos/bovespa-winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ingest_data('urls.csv', '../data/00_raw/', time_to_sleep=3, store_locally=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fundamentus.get_resultado().reset_index().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get detailed information about each ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = df['papel'].unique()\n",
    "\n",
    "df_tickers = get_detailed_ticker_data(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tickers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Storing the raw data into Google Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload raw fundamentalist data to Google Cloud Storage\n",
    "blob = bucket.blob('01_raw/fundamentus_historical_raw.csv')\n",
    "blob.upload_from_string(df.to_csv(), 'text/csv')\n",
    "# # upload raw fundamentalist data to Google Cloud Storage\n",
    "# blob = bucket.blob('01_raw/fundamentus_tickers_raw.csv')\n",
    "# blob.upload_from_string(df_tickers.to_csv(), 'text/csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"gs://storage-barsianize/01_raw/fundamentus_historical_raw.csv\"\n",
    "df =  pd.read_csv(path, index_col=0)\n",
    "\n",
    "path = \"gs://storage-barsianize/01_raw/fundamentus_tickers_raw.csv\"\n",
    "df_tickers = pd.read_csv(path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "columns_to_spline = ['cotacao',\n",
    "                     'pl',\n",
    "                     'pvp',\n",
    "                     'psr',\n",
    "                     'dy',\n",
    "                     'pa',\n",
    "                     'pcg',\n",
    "                     'pebit',\n",
    "                     'pacl',\n",
    "                     'evebit',\n",
    "                     'mrgebit',\n",
    "                     'mrgliq',\n",
    "                     'roic',\n",
    "                     'roe',\n",
    "                     'liqc',\n",
    "                     'liq2m',\n",
    "                     'patrliq',\n",
    "                     'divbpatr',\n",
    "                     'c5y']\n",
    "\n",
    "columns_to_first = ['papel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily = get_daily_data_per_ticker(df, 'papel', columns_to_first, columns_to_spline, 'date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Storing the raw data into Google Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_local = '../data/01_trusted/'\n",
    "filename = 'daily_data.parquet'\n",
    "blob_name = '02_trusted/'\n",
    "\n",
    "# save the DataFrame as a parquet file\n",
    "df_daily.to_parquet(path_local + filename)\n",
    "\n",
    "# upload the parquet file to Google Cloud Storage\n",
    "blob = bucket.blob(blob_name + filename)\n",
    "blob._chunk_size = 8388608\n",
    "blob.upload_from_filename(path_local + filename, num_retries=10, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"gs://storage-barsianize/02_trusted/daily_data.parquet\"\n",
    "df_daily =  pd.read_parquet(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being sure that there are no negative DY values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily['dy'][df_daily['dy']<0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking out all the tickers that were not active before 2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tickers = df_tickers[~(pd.to_datetime(df_tickers['Data_ult_cot']).dt.year<2008)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating numerical categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tickers = create_numerical_categories(df_tickers, ['Valor_de_mercado','Lucro_Liquido_12m','Receita_Liquida_12m','Patrim_Liq'], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a BESST 1 and BESST 2 features for the companies that belong to specific sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "besst_1 =  [\n",
    "    'Intermediários Financeiros',\n",
    "    'Energia Elétrica',\n",
    "    'Previdência e Seguros',\n",
    "    'Água e Saneamento',\n",
    "    'Serviços Financeiros Diversos'\n",
    "]\n",
    "\n",
    "besst_2 =  [\n",
    "    'Mineração',\n",
    "    'Madeira e Papel',\n",
    "    'Químicos',\n",
    "    'Siderurgia e Metalurgia',\n",
    "    'Petróleo, Gás e Biocombustíveis'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df_tickers[np.isin(df_tickers['Setor'], besst_1)]['Lucro_Liquido_12m_category'])\n",
    "sns.displot(df_tickers[np.isin(df_tickers['Setor'], besst_2)]['Lucro_Liquido_12m_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tickers = create_besst_categories(df_tickers, 'Setor', besst_1, besst_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correcting DY values from str (in %) to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tickers['Div_Yield'] = df_tickers['Div_Yield'].apply(from_percent_to_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting just the tickers with the type we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tickers['Tipo'] = df_tickers['Tipo'].str[:2]\n",
    "df_tickers = df_tickers.loc[np.isin(df_tickers['Tipo'],['ON','PN'])]\n",
    "df_tickers = df_tickers.loc[np.isin(df_tickers['Papel'].str[-1],['3','4','5','6'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating completed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_tickers = ['Papel','Tipo', 'Empresa', 'Setor', 'Subsetor','Data_ult_cot','Lucro_Liquido_12m_category','Valor_de_mercado_category','Patrim_Liq_category','besst_1','besst_2']\n",
    "               \n",
    "df_tickers_clean = df_tickers[info_tickers]\n",
    "\n",
    "df_tickers_clean = df_tickers_clean.dropna()\n",
    "df_tickers_clean = df_tickers_clean.drop_duplicates()\n",
    "\n",
    "df_tickers_clean['Data_ult_cot'] = pd.to_datetime(df_tickers_clean['Data_ult_cot'])\n",
    "df_tickers_clean = df_tickers_clean.reset_index(drop=True)\n",
    "\n",
    "df_completed = df_tickers_clean.merge(df_daily, how='left', left_on='Papel', right_on='papel').drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completed.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client, bucket = initialize_bucket(credentials_path,'storage-barsianize')\n",
    "\n",
    "path_local = '../data/02_refined/'\n",
    "filename = 'df_completed_daily.parquet'\n",
    "blob_name = '03_refined/'\n",
    "\n",
    "# save the DataFrame as a parquet file\n",
    "df_completed.to_parquet(path_local + filename)\n",
    "\n",
    "# upload the parquet file to Google Cloud Storage\n",
    "blob = bucket.blob(blob_name + filename)\n",
    "blob._chunk_size = 8388608\n",
    "blob.upload_from_filename(path_local + filename, num_retries=10, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client, bucket = initialize_bucket(credentials_path,'storage-barsianize')\n",
    "\n",
    "path = \"gs://storage-barsianize/03_refined/df_completed_daily.parquet\"\n",
    "df_completed =  pd.read_parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_col = 'Papel'\n",
    "date_col = 'date'\n",
    "tickers = list(df_completed[ticker_col].unique())\n",
    "agg_dict = {\n",
    "        'Papel':'first',\n",
    "        'Tipo':'first',\n",
    "        'Empresa':'first',\n",
    "        'Setor':'first',\n",
    "        'Subsetor':'first',\n",
    "        'Data_ult_cot':'first',\n",
    "        'Lucro_Liquido_12m_category':'first',\n",
    "        'Valor_de_mercado_category':'first',\n",
    "        'Patrim_Liq_category':'first',\n",
    "        'besst_1':'first',\n",
    "        'besst_2':'first',\n",
    "        'cotacao':['max','min','mean','last'],\n",
    "        'pl':['max','min','mean','last'],\n",
    "        'pvp':['max','min','mean','last'],\n",
    "        'psr':['max','min','mean','last'],\n",
    "        'dy':['max','min','mean','last','median'],\n",
    "        'pa':['max','min','mean','last'],\n",
    "        'pcg':['max','min','mean','last'],\n",
    "        'pebit':['max','min','mean','last'],\n",
    "        'pacl':['max','min','mean','last'],\n",
    "        'evebit':['max','min','mean','last'],\n",
    "        'mrgebit':['max','min','mean','last'],\n",
    "        'mrgliq':['max','min','mean','last'],\n",
    "        'roic':['max','min','mean','last'],\n",
    "        'roe':['max','min','mean','last'],\n",
    "        'liqc':['max','min','mean','last'],\n",
    "        'liq2m':['max','min','mean','last'],\n",
    "        'patrliq':['max','min','mean','last'],\n",
    "        'divbpatr':['max','min','mean','last'],\n",
    "        'c5y':['max','min','mean','last'],\n",
    "        'date':'last',\n",
    "        'year':'first'\n",
    "        }\n",
    "\n",
    "df_agg_monthly = aggregator_monthly(df_completed, tickers, ticker_col, date_col, agg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_monthly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Central Bank Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www3.bcb.gov.br/sgspub/localizarseries/localizarSeries.do?method=prepararTelaLocalizarSeries\n",
    "\n",
    "codes = {\n",
    "    'preco_do_petroleo': 4390,\n",
    "    'preco_do_minerio_de_ferro': 25521,\n",
    "    'indice_da_industria': 24369,\n",
    "    'indice_do_agro': 24368,\n",
    "    'dolar_comercial': 1,\n",
    "    'euro': 21619,\n",
    "    'ibovespa': 23686,\n",
    "    'pib': 21920,\n",
    "    'pib_dolarizado': 22786,\n",
    "    'igpm': 189,\n",
    "    'ipca': 433,\n",
    "    'selic': 11\n",
    "}\n",
    "start_date = \"01/01/2008\"\n",
    "end_date = \"28/02/2023\"\n",
    "output_format='json'\n",
    "\n",
    "df_bcb = get_bcb_data(codes, start_date, end_date, output_format=output_format)\n",
    "\n",
    "df_bcb['data'] = pd.to_datetime(df_bcb['data'].apply(correct_data_column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bcb = df_bcb.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "agg_dict = {\n",
    "        'data':'first',\n",
    "        # 'preco_do_petroleo':'last',\n",
    "        'indice_da_industria':'last',\n",
    "        'dolar_comercial':'last',\n",
    "        'euro':'last',\n",
    "        'ibovespa':'last',\n",
    "        'pib_dolarizado':'last',\n",
    "        'igpm':'last',\n",
    "        'ipca':'last',\n",
    "        'selic':'last',\n",
    "        }\n",
    "df_bcb_agg = df_bcb.resample('M', on='data').agg(agg_dict)\n",
    "df_bcb_agg = df_bcb_agg.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bcb_agg['month'] = df_bcb_agg['data'].dt.month\n",
    "df_bcb_agg['year'] = df_bcb_agg['data'].dt.year\n",
    "\n",
    "df_agg_monthly['month'] = df_agg_monthly['date_last'].dt.month\n",
    "\n",
    "df_monthly_full = df_agg_monthly.merge(df_bcb_agg, how='left', left_on=['month','year'], right_on=['month','year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client, bucket = initialize_bucket(credentials_path,'storage-barsianize')\n",
    "\n",
    "path_local = '../data/02_refined/'\n",
    "filename = 'df_monthly_full.parquet'\n",
    "blob_name = '03_refined/'\n",
    "\n",
    "# save the DataFrame as a parquet file\n",
    "df_monthly_full.to_parquet(path_local + filename)\n",
    "\n",
    "# upload the parquet file to Google Cloud Storage\n",
    "blob = bucket.blob(blob_name + filename)\n",
    "blob._chunk_size = 8388608\n",
    "blob.upload_from_filename(path_local + filename, num_retries=10, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create windowed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"gs://storage-barsianize/03_refined/df_monthly_full.parquet\"\n",
    "df_monthly_full =  pd.read_parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monthly_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_dict = {\n",
    "    'cotacao_max':'max',\t\n",
    "    'cotacao_min':'min',\n",
    "    'cotacao_mean':['mean','std'],\n",
    "    'pl_max':'max',\n",
    "    'pl_min':'min',\n",
    "    'pl_mean':['mean','std'],\n",
    "    'pvp_max':'max',\n",
    "    'pvp_min':'min',\n",
    "    'pvp_mean':['mean','std'],\n",
    "    'psr_max':'max',\n",
    "    'psr_min':'min',\n",
    "    'psr_mean':['mean','std'],\n",
    "    'dy_max':'max',\n",
    "    'dy_min':'min',\n",
    "    'dy_median':['max','min'],\n",
    "    'dy_mean':['mean','std'],\n",
    "    'pa_max':'max',\n",
    "    'pa_min':'min',\n",
    "    'pa_mean':['mean','std'],\n",
    "    'pcg_max':'max',\n",
    "    'pcg_min':'min',\n",
    "    'pcg_mean':['mean','std'],\n",
    "    'pebit_max':'max',\n",
    "    'pebit_min':'min',\n",
    "    'pebit_mean':['mean','std'],\n",
    "    'pacl_max':'max',\n",
    "    'pacl_min':'min',\n",
    "    'pacl_mean':['mean','std'],\n",
    "    'evebit_max':'max',\n",
    "    'evebit_min':'min',\n",
    "    'evebit_mean':['mean','std'],\n",
    "    'mrgebit_max':'max',\n",
    "    'mrgebit_min':'min',\n",
    "    'mrgebit_mean':['mean','std'],\n",
    "    'mrgliq_max':'max',\n",
    "    'mrgliq_min':'min',\n",
    "    'mrgliq_mean':['mean','std'],\n",
    "    'roic_max':'max',\n",
    "    'roic_min':'min',\n",
    "    'roic_mean':['mean','std'],\n",
    "    'roe_max':'max',\n",
    "    'roe_min':'min',\n",
    "    'roe_mean':['mean','std'],\n",
    "    'liqc_max':'max',\n",
    "    'liqc_min':'min',\n",
    "    'liqc_mean':['mean','std'],\n",
    "    'liq2m_max':'max',\n",
    "    'liq2m_min':'min',\n",
    "    'liq2m_mean':['mean','std'],\n",
    "    'patrliq_max':'max',\n",
    "    'patrliq_min':'min',\n",
    "    'patrliq_mean':['mean','std'],\n",
    "    'divbpatr_max':'max',\n",
    "    'divbpatr_min':'min',\n",
    "    'divbpatr_mean':['mean','std'],\n",
    "    'c5y_max':'max',\n",
    "    'c5y_min':'min',\n",
    "    'c5y_mean':['mean','std'],\n",
    "    # 'preco_do_petroleo':['min','max','mean','std'],\n",
    "    'indice_da_industria':['min','max','mean','std'],\n",
    "    'dolar_comercial':['min','max','mean','std'],\n",
    "    'euro':['min','max','mean','std'],\n",
    "    'ibovespa':['min','max','mean','std'],\n",
    "    'pib_dolarizado':['min','max','mean','std'],\n",
    "    'igpm':['min','max','mean','std'],\n",
    "    'ipca':['min','max','mean','std'],\n",
    "    'selic':['min','max','mean','std'],\n",
    "}\n",
    "\n",
    "agg_dict_ref = {\n",
    "    'cotacao_max_ref':'max',\t\n",
    "    'cotacao_min_ref':'min',\n",
    "    'cotacao_mean_ref':['mean','std'],\n",
    "    'pl_max_ref':'max',\n",
    "    'pl_min_ref':'min',\n",
    "    'pl_mean_ref':['mean','std'],\n",
    "    'pvp_max_ref':'max',\n",
    "    'pvp_min_ref':'min',\n",
    "    'pvp_mean_ref':['mean','std'],\n",
    "    'psr_max_ref':'max',\n",
    "    'psr_min_ref':'min',\n",
    "    'psr_mean_ref':['mean','std'],\n",
    "    'dy_max_ref':'max',\n",
    "    'dy_min_ref':'min',\n",
    "    'dy_median_ref':['max','min'],\n",
    "    'dy_mean_ref':['mean','std'],\n",
    "    'pa_max_ref':'max',\n",
    "    'pa_min_ref':'min',\n",
    "    'pa_mean_ref':['mean','std'],\n",
    "    'pcg_max_ref':'max',\n",
    "    'pcg_min_ref':'min',\n",
    "    'pcg_mean_ref':['mean','std'],\n",
    "    'pebit_max_ref':'max',\n",
    "    'pebit_min_ref':'min',\n",
    "    'pebit_mean_ref':['mean','std'],\n",
    "    'pacl_max_ref':'max',\n",
    "    'pacl_min_ref':'min',\n",
    "    'pacl_mean_ref':['mean','std'],\n",
    "    'evebit_max_ref':'max',\n",
    "    'evebit_min_ref':'min',\n",
    "    'evebit_mean_ref':['mean','std'],\n",
    "    'mrgebit_max_ref':'max',\n",
    "    'mrgebit_min_ref':'min',\n",
    "    'mrgebit_mean_ref':['mean','std'],\n",
    "    'mrgliq_max_ref':'max',\n",
    "    'mrgliq_min_ref':'min',\n",
    "    'mrgliq_mean_ref':['mean','std'],\n",
    "    'roic_max_ref':'max',\n",
    "    'roic_min_ref':'min',\n",
    "    'roic_mean_ref':['mean','std'],\n",
    "    'roe_max_ref':'max',\n",
    "    'roe_min_ref':'min',\n",
    "    'roe_mean_ref':['mean','std'],\n",
    "    'liqc_max_ref':'max',\n",
    "    'liqc_min_ref':'min',\n",
    "    'liqc_mean_ref':['mean','std'],\n",
    "    'liq2m_max_ref':'max',\n",
    "    'liq2m_min_ref':'min',\n",
    "    'liq2m_mean_ref':['mean','std'],\n",
    "    'patrliq_max_ref':'max',\n",
    "    'patrliq_min_ref':'min',\n",
    "    'patrliq_mean_ref':['mean','std'],\n",
    "    'divbpatr_max_ref':'max',\n",
    "    'divbpatr_min_ref':'min',\n",
    "    'divbpatr_mean_ref':['mean','std'],\n",
    "    'c5y_max_ref':'max',\n",
    "    'c5y_min_ref':'min',\n",
    "    'c5y_mean_ref':['mean','std'],\n",
    "}\n",
    "\n",
    "columns_to_first = [\n",
    "    'Papel',\n",
    "    'Tipo',\n",
    "    'Empresa',\n",
    "    'Setor',\n",
    "    'Subsetor',\n",
    "    'Data_ult_cot',\n",
    "    'Lucro_Liquido_12m_category',\n",
    "    'Valor_de_mercado_category',\n",
    "    'Patrim_Liq_category',\n",
    "    'besst_1',\n",
    "    'besst_2',\n",
    "]\n",
    "\n",
    "columns_to_last = [\n",
    "    'year',\n",
    "    'date_last',\n",
    "    'month',\n",
    "    # 'preco_do_petroleo',\n",
    "    'indice_da_industria',\n",
    "    'dolar_comercial',\n",
    "    'euro',\n",
    "    'ibovespa',\n",
    "    'pib_dolarizado',\n",
    "    'igpm',\n",
    "    'ipca',\n",
    "    'selic',\n",
    "    'cotacao_mean',\n",
    "    'c5y_mean',\n",
    "    'pl_mean',\n",
    "    'pvp_mean',\n",
    "    'psr_mean',\n",
    "    'dy_mean',\n",
    "    'dy_median',\n",
    "    'pa_mean',\n",
    "    'pcg_mean',\n",
    "    'pebit_mean',\n",
    "    'pacl_mean',\n",
    "    'evebit_mean',\n",
    "    'mrgebit_mean',\n",
    "    'mrgliq_mean',\n",
    "    'roic_mean',\n",
    "    'roe_mean',\n",
    "    'liqc_mean',\n",
    "    'liq2m_mean',\n",
    "    'patrliq_mean',\n",
    "    'divbpatr_mean',\n",
    "]\n",
    "\n",
    "sector_col = 'Setor'\n",
    "cols_to_ref = [\n",
    "    'cotacao_max',\n",
    "    'cotacao_min',\n",
    "    'cotacao_mean',\n",
    "    'cotacao_last',\n",
    "    'pl_max',\n",
    "    'pl_min',\n",
    "    'pl_mean',\n",
    "    'pl_last',\n",
    "    'pvp_max',\n",
    "    'pvp_min',\n",
    "    'pvp_mean',\n",
    "    'pvp_last',\n",
    "    'psr_max',\n",
    "    'psr_min',\n",
    "    'psr_mean',\n",
    "    'psr_last',\n",
    "    'dy_max',\n",
    "    'dy_min',\n",
    "    'dy_mean',\n",
    "    'dy_median',\n",
    "    'dy_last',\n",
    "    'pa_max',\n",
    "    'pa_min',\n",
    "    'pa_mean',\n",
    "    'pa_last',\n",
    "    'pcg_max',\n",
    "    'pcg_min',\n",
    "    'pcg_mean',\n",
    "    'pcg_last',\n",
    "    'pebit_max',\n",
    "    'pebit_min',\n",
    "    'pebit_mean',\n",
    "    'pebit_last',\n",
    "    'pacl_max',\n",
    "    'pacl_min',\n",
    "    'pacl_mean',\n",
    "    'pacl_last',\n",
    "    'evebit_max',\n",
    "    'evebit_min',\n",
    "    'evebit_mean',\n",
    "    'evebit_last',\n",
    "    'mrgebit_max',\n",
    "    'mrgebit_min',\n",
    "    'mrgebit_mean',\n",
    "    'mrgebit_last',\n",
    "    'mrgliq_max',\n",
    "    'mrgliq_min',\n",
    "    'mrgliq_mean',\n",
    "    'mrgliq_last',\n",
    "    'roic_max',\n",
    "    'roic_min',\n",
    "    'roic_mean',\n",
    "    'roic_last',\n",
    "    'roe_max',\n",
    "    'roe_min',\n",
    "    'roe_mean',\n",
    "    'roe_last',\n",
    "    'liqc_max',\n",
    "    'liqc_min',\n",
    "    'liqc_mean',\n",
    "    'liqc_last',\n",
    "    'liq2m_max',\n",
    "    'liq2m_min',\n",
    "    'liq2m_mean',\n",
    "    'liq2m_last',\n",
    "    'patrliq_max',\n",
    "    'patrliq_min',\n",
    "    'patrliq_mean',\n",
    "    'patrliq_last',\n",
    "    'divbpatr_max',\n",
    "    'divbpatr_min',\n",
    "    'divbpatr_mean',\n",
    "    'divbpatr_last',\n",
    "    'c5y_max',\n",
    "    'c5y_min',\n",
    "    'c5y_mean',\n",
    "    'c5y_last'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monthly_full_ref = df_monthly_full.groupby([sector_col,'year','month']).median()[cols_to_ref]\n",
    "df_monthly_full_ref.columns = [col+'_ref' for col in df_monthly_full_ref.columns]\n",
    "df_monthly_full_ref = df_monthly_full_ref.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monthly_full_merge =  df_monthly_full.merge(df_monthly_full_ref, how='left', on=['Setor','year','month'])\n",
    "ref_cols = [col for col in df_monthly_full_merge.columns if '_ref' in col]\n",
    "val_cols = [col[:-4] for col in ref_cols]\n",
    "\n",
    "df_monthly_full_merge[ref_cols] = np.divide(df_monthly_full_merge[val_cols],df_monthly_full_merge[ref_cols])\n",
    "\n",
    "df_monthly_full_merge.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monthly_full_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_col = 'Papel'\n",
    "tickers = df_monthly_full[ticker_col].unique()\n",
    "agg_dict_ref = agg_dict_ref\n",
    "\n",
    "df_windowed_full = get_windowed_data(df_monthly_full_merge, agg_dict, tickers, ticker_col,columns_to_first,columns_to_last,agg_dict_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_windowed_full.dropna(inplace=True)\n",
    "df_windowed_full.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add window spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cols = [col for col in df_windowed_full.columns if '_max' in col]\n",
    "min_cols = [col for col in df_windowed_full.columns if '_min' in col]\n",
    "spread_cols = [re.sub('mean_','',re.sub('_min','',re.sub('_max','',col)))+'_spread' for col in max_cols]\n",
    "print(spread_cols)\n",
    "\n",
    "df_windowed_full[spread_cols] = np.subtract(df_windowed_full[max_cols],df_windowed_full[min_cols])\n",
    "\n",
    "spread_relevance_cols = [re.sub('mean_','',re.sub('_min','',re.sub('_max','',col)))+'_spread' for col in max_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_cols = [col for col in df_windowed_full.columns if '_last' in col]\n",
    "\n",
    "spread_cols_clean = [re.sub('_spread','',col) for col in spread_cols]\n",
    "last_cols_clean = [re.sub('_mean','',re.sub('_last','',col)) for col in df_windowed_full.columns if '_last' in col]\n",
    "\n",
    "spread_cols = np.array(spread_cols)\n",
    "last_cols = np.array(last_cols)\n",
    "\n",
    "spread_relevance_cols_spread = sorted(spread_cols[np.isin(spread_cols_clean,last_cols_clean)])\n",
    "spread_relevance_cols_last   = sorted(last_cols[np.isin(last_cols_clean,spread_cols_clean)])\n",
    "\n",
    "spread_relevance_cols = [col + '_relevance' for col in spread_relevance_cols_spread]\n",
    "\n",
    "df_windowed_full[spread_relevance_cols] = np.divide(df_windowed_full[spread_relevance_cols_spread],df_windowed_full[spread_relevance_cols_last]).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_windowed_full[df_windowed_full['Papel']=='BBAS3'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing refined data into Google Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'initialize_bucket' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Pontello\\repos\\DataScience_Nanodegree\\06_Capstone_Project\\notebooks\\00_data_eng.ipynb Cell 77\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Pontello/repos/DataScience_Nanodegree/06_Capstone_Project/notebooks/00_data_eng.ipynb#Y150sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m client, bucket \u001b[39m=\u001b[39m initialize_bucket(credentials_path,\u001b[39m'\u001b[39m\u001b[39mstorage-barsianize\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Pontello/repos/DataScience_Nanodegree/06_Capstone_Project/notebooks/00_data_eng.ipynb#Y150sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m path_local \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m../data/02_refined/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Pontello/repos/DataScience_Nanodegree/06_Capstone_Project/notebooks/00_data_eng.ipynb#Y150sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdf_monthly_full_ref.parquet\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'initialize_bucket' is not defined"
     ]
    }
   ],
   "source": [
    "client, bucket = initialize_bucket(credentials_path,'storage-barsianize')\n",
    "\n",
    "path_local = '../data/02_refined/'\n",
    "filename = 'df_monthly_full_ref.parquet'\n",
    "blob_name = '03_refined/'\n",
    "\n",
    "# save the DataFrame as a parquet file\n",
    "df_monthly_full_ref.to_parquet(path_local + filename)\n",
    "\n",
    "# upload the parquet file to Google Cloud Storage\n",
    "blob = bucket.blob(blob_name + filename)\n",
    "blob._chunk_size = 8388608\n",
    "blob.upload_from_filename(path_local + filename, num_retries=10, )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path_local = '../data/02_refined/'\n",
    "filename = 'df_monthly_full_merge.parquet'\n",
    "blob_name = '03_refined/'\n",
    "\n",
    "# save the DataFrame as a parquet file\n",
    "df_monthly_full_merge.to_parquet(path_local + filename)\n",
    "\n",
    "# upload the parquet file to Google Cloud Storage\n",
    "blob = bucket.blob(blob_name + filename)\n",
    "blob._chunk_size = 8388608\n",
    "blob.upload_from_filename(path_local + filename, num_retries=10, )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path_local = '../data/02_refined/'\n",
    "filename = 'df_windowed_full.parquet'\n",
    "blob_name = '03_refined/'\n",
    "\n",
    "# save the DataFrame as a parquet file\n",
    "df_windowed_full.to_parquet(path_local + filename)\n",
    "\n",
    "# upload the parquet file to Google Cloud Storage\n",
    "blob = bucket.blob(blob_name + filename)\n",
    "blob._chunk_size = 8388608\n",
    "blob.upload_from_filename(path_local + filename, num_retries=10, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello world!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
